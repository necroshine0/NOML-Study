{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCfOAvvpXHaH"
   },
   "source": [
    "# –í–≤–µ–¥–µ–Ω–∏–µ –≤ –≥–ª—É–±–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –§–ö–ù –í–®–≠\n",
    "\n",
    "## –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.\n",
    "\n",
    "### –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n",
    "\n",
    "–î–∞—Ç–∞ –≤—ã–¥–∞—á–∏: 13.01.2022\n",
    "\n",
    "–ú—è–≥–∫–∏–π –¥–µ–¥–ª–∞–π–Ω: 23:59MSK 6.02.2022\n",
    "\n",
    "–ñ–µ—Å—Ç–∫–∏–π –¥–µ–¥–ª–∞–π–Ω: 23:59MSK 10.02.2022\n",
    "\n",
    "–û—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ —à—Ç—Ä–∞—Ñ–∞ –ø–æ—Å–ª–µ –º—è–≥–∫–æ–≥–æ –¥–µ–¥–ª–∞–π–Ω–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ $M_{penalty} = M_{full} \\cdot 0.85^{t/1440}$, –≥–¥–µ $M_{full}$ ‚Äî –ø–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞ —Ä–∞–±–æ—Ç—É –±–µ–∑ —É—á–µ—Ç–∞ —à—Ç—Ä–∞—Ñ–∞, –∞ $t$ ‚Äî –≤—Ä–µ–º—è –≤ –º–∏–Ω—É—Ç–∞—Ö, –ø—Ä–æ—à–µ–¥—à–µ–µ –ø–æ—Å–ª–µ –º—è–≥–∫–æ–≥–æ –¥–µ–¥–ª–∞–π–Ω–∞ (–æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ –¥–≤—É—Ö —Ü–∏—Ñ—Ä –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å–ø—É—Å—Ç—è –ø–µ—Ä–≤—ã–µ —Å—É—Ç–∫–∏ –ø–æ—Å–ª–µ –º—è–≥–∫–æ–≥–æ –¥–µ–¥–ª–∞–π–Ω–∞ –≤—ã –Ω–µ –º–æ–∂–µ—Ç–µ –ø–æ–ª—É—á–∏—Ç—å –æ—Ü–µ–Ω–∫—É –≤—ã—à–µ 8.5, –∞ –µ—Å–ª–∏ —Å–¥–∞—Ç—å –ø–µ—Ä–µ–¥ —Å–∞–º—ã–º –∂–µ—Å—Ç–∫–∏–º –¥–µ–¥–ª–∞–π–Ω–æ–º, —Ç–æ –≤–∞—à –º–∞–∫—Å–∏–º—É–º ‚Äî 5.22 –±–∞–ª–ª–∞.\n",
    "\n",
    "### –û—Ü–µ–Ω–∏–≤–∞–Ω–∏–µ –∏ —à—Ç—Ä–∞—Ñ—ã\n",
    "\n",
    "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞ —Ä–∞–±–æ—Ç—É ‚Äî 10 –±–∞–ª–ª–æ–≤. –°–¥–∞–≤–∞—Ç—å –∑–∞–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å—Ä–æ–∫–∞ —Å–¥–∞—á–∏ –Ω–µ–ª—å–∑—è.\n",
    "\n",
    "–ó–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. ¬´–ü–æ—Ö–æ–∂–∏–µ¬ª —Ä–µ—à–µ–Ω–∏—è —Å—á–∏—Ç–∞—é—Ç—Å—è –ø–ª–∞–≥–∏–∞—Ç–æ–º –∏ –≤—Å–µ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—É–¥–µ–Ω—Ç—ã (–≤ —Ç–æ–º —á–∏—Å–ª–µ —Ç–µ, —É –∫–æ–≥–æ —Å–ø–∏—Å–∞–ª–∏) –Ω–µ –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –∑–∞ –Ω–µ–≥–æ –±–æ–ª—å—à–µ 0 –±–∞–ª–ª–æ–≤. –ï—Å–ª–∏ –≤—ã –Ω–∞—à–ª–∏ —Ä–µ—à–µ–Ω–∏–µ –∫–∞–∫–æ–≥–æ-—Ç–æ –∏–∑ –∑–∞–¥–∞–Ω–∏–π (–∏–ª–∏ –µ–≥–æ —á–∞—Å—Ç—å) –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ —ç—Ç–æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –±–ª–æ–∫–µ –≤ –∫–æ–Ω—Ü–µ –≤–∞—à–µ–π —Ä–∞–±–æ—Ç—ã (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –≤—ã –±—É–¥–µ—Ç–µ –Ω–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º, –∫—Ç–æ —ç—Ç–æ –Ω–∞—à–µ–ª, –ø–æ—ç—Ç–æ–º—É —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–µ –≤ –ø–ª–∞–≥–∏–∞—Ç–µ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫).\n",
    "\n",
    "–ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞ –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ –æ—Ç—Ä–∞–∑–∏—Ç—å—Å—è –Ω–∞ –æ—Ü–µ–Ω–∫–µ. –¢–∞–∫–∂–µ –æ—Ü–µ–Ω–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–Ω–∏–∂–µ–Ω–∞ –∑–∞ –ø–ª–æ—Ö–æ —á–∏—Ç–∞–µ–º—ã–π –∫–æ–¥ –∏ –ø–ª–æ—Ö–æ –æ—Ñ–æ—Ä–º–ª–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏. –í—Å–µ –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞—Ç—å—Å—è –∫–æ–¥–æ–º –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ –æ —Ç–æ–º, –∫–∞–∫ –æ–Ω–∏ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã.\n",
    "\n",
    "### –û –∑–∞–¥–∞–Ω–∏–∏\n",
    "\n",
    "–í –¥–∞–Ω–Ω–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –æ—Ç–µ–ª—è –ø–æ —Ç–µ–∫—Å—Ç—É –æ—Ç–∑—ã–≤–∞. –ù—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç–≥–≥–ª–∞ –∏ –∑–∞—Å–ª–∞—Ç—å –≤ [—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ](https://www.kaggle.com/t/3e8fa6cec6d048bf8e93fb72e441d88c) –ø—Ä–µ–¥–∏–∫—Ç. –ü–æ —Ç–æ–π –∂–µ —Å—Å—ã–ª–∫–µ –º–æ–∂–µ—Ç–µ —Å–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.\n",
    "\n",
    "–ú—ã —Å–æ–±—Ä–∞–ª–∏ –¥–ª—è –≤–∞—Å –æ—Ç–∑—ã–≤—ã –ø–æ 1500 –æ—Ç–µ–ª—è–º –∏–∑ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ —Ä–∞–∑–Ω—ã—Ö —É–≥–æ–ª–∫–æ–≤ –º–∏—Ä–∞. –ß—Ç–æ —ç—Ç–æ –∑–∞ –æ—Ç–µ–ª–∏ - —Å–µ–∫—Ä–µ—Ç. –í–∞–º –¥–∞–Ω —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Ç–µ–ª—è. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ - –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ—Ü–µ–Ω–∫—É –æ—Ç–µ–ª—è –ø–æ –æ—Ç–∑—ã–≤—É.\n",
    "\n",
    "–ì–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - Mean Absolute Error (MAE). –í–æ –≤—Å–µ—Ö —á–∞—Å—Ç—è—Ö –¥–æ–º–∞—à–Ω–µ–π —Ä–∞–±–æ—Ç—ã –≤–∞–º –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ MAE –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—â–µ–µ 0.92 –Ω–∞ –ø—É–±–ª–∏—á–Ω–æ–º –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –º—ã –±—É–¥–µ–º –≤—ã–Ω—É–∂–¥–µ–Ω—ã –Ω–µ –∑–∞—Å—á–∏—Ç–∞—Ç—å –∑–∞–¥–∞–Ω–∏–µ :( \n",
    "\n",
    "#### –ü—Ä–æ –¥–∞–Ω–Ω—ã–µ:\n",
    "–ö–∞–∂–¥–æ–µ —Ä–µ–≤—å—é —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤: positive –∏ negative - –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã –æ—Ç–µ–ª—è. –í —Å—Ç–æ–ª–±—Ü–µ score –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è - –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ 0 –¥–æ 10. –í–∞–º –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —ç—Ç–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø–æ –Ω–∏–º –æ—Ü–µ–Ω–∫—É.\n",
    "\n",
    "–î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–π–Ω –∏ —Ç–µ—Å—Ç.\n",
    "\n",
    "Good luck & have fun! üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter\n",
    "\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6Ej16t1XHaM"
   },
   "source": [
    "#### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫—Ä–æ–º–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏ —Å—Ç—Ä–æ–≥–æ –∑–∞–ø—Ä–µ—â–µ–Ω–æ. –í –ø–æ—Å–ª–µ–¥–Ω–µ–π —á–∞—Å—Ç–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U4Gc4Go5XHaN"
   },
   "outputs": [],
   "source": [
    "PATH_TO_TRAIN_DATA = 'data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6kJRM6ZUXHaO",
    "outputId": "e6ba4512-21c3-4dd8-a1d5-e270541cdc64"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30470</th>\n",
       "      <td>4e7190aa04bf9d16404b0542b0497c2e</td>\n",
       "      <td>Quite expensive</td>\n",
       "      <td>Really helpful polite staff Good quality d co...</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40627</th>\n",
       "      <td>6839cf76a9497590d2c08d7bdd420e86</td>\n",
       "      <td>Nothing at all</td>\n",
       "      <td>Location is unbeatable in walkable distance a...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19050</th>\n",
       "      <td>31642d36322487ae0db4ca12e30618f0</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>The room was great and the staff was lovely s...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75833</th>\n",
       "      <td>c23d6eb9f80f40090c53018afc5be86f</td>\n",
       "      <td>ceiling height compromised on floor 6 overloo...</td>\n",
       "      <td>great staff</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11050</th>\n",
       "      <td>1ca844ff439b9c966b5e11609ba660a2</td>\n",
       "      <td>The breakfast no variety of cheeses mainly it...</td>\n",
       "      <td>Location the room lunch in the room</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85176</th>\n",
       "      <td>d9c124e0a6cc4aeef21b036b871395c8</td>\n",
       "      <td>No Negative</td>\n",
       "      <td>Everything the hotel is pur class</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88925</th>\n",
       "      <td>e3631610790276e7d842c5eb47339efc</td>\n",
       "      <td>No Negative</td>\n",
       "      <td>Beds extremely comfortable Staff were very we...</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69829</th>\n",
       "      <td>b2eae52da76992a1808c6deb9c88e201</td>\n",
       "      <td>Only Spanish TV channels reception under char...</td>\n",
       "      <td>Easy to find close to shops and 2 major tube ...</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77036</th>\n",
       "      <td>c5391e0c528002eaea481ee9477ef7db</td>\n",
       "      <td>Size of the room was small</td>\n",
       "      <td>The hotel was in a good location but the room...</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39941</th>\n",
       "      <td>667f1cbaba1333c8fe7f39261d9bac8b</td>\n",
       "      <td>a little bit expensive but not a big gap comp...</td>\n",
       "      <td>The food at breakfast was really good a lot o...</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              review_id  \\\n",
       "30470  4e7190aa04bf9d16404b0542b0497c2e   \n",
       "40627  6839cf76a9497590d2c08d7bdd420e86   \n",
       "19050  31642d36322487ae0db4ca12e30618f0   \n",
       "75833  c23d6eb9f80f40090c53018afc5be86f   \n",
       "11050  1ca844ff439b9c966b5e11609ba660a2   \n",
       "85176  d9c124e0a6cc4aeef21b036b871395c8   \n",
       "88925  e3631610790276e7d842c5eb47339efc   \n",
       "69829  b2eae52da76992a1808c6deb9c88e201   \n",
       "77036  c5391e0c528002eaea481ee9477ef7db   \n",
       "39941  667f1cbaba1333c8fe7f39261d9bac8b   \n",
       "\n",
       "                                                negative  \\\n",
       "30470                                   Quite expensive    \n",
       "40627                                    Nothing at all    \n",
       "19050                                           Nothing    \n",
       "75833   ceiling height compromised on floor 6 overloo...   \n",
       "11050   The breakfast no variety of cheeses mainly it...   \n",
       "85176                                        No Negative   \n",
       "88925                                        No Negative   \n",
       "69829   Only Spanish TV channels reception under char...   \n",
       "77036                        Size of the room was small    \n",
       "39941   a little bit expensive but not a big gap comp...   \n",
       "\n",
       "                                                positive  score  \n",
       "30470   Really helpful polite staff Good quality d co...    9.6  \n",
       "40627   Location is unbeatable in walkable distance a...   10.0  \n",
       "19050   The room was great and the staff was lovely s...   10.0  \n",
       "75833                                        great staff    7.9  \n",
       "11050                Location the room lunch in the room    9.6  \n",
       "85176                  Everything the hotel is pur class    9.6  \n",
       "88925   Beds extremely comfortable Staff were very we...    9.2  \n",
       "69829   Easy to find close to shops and 2 major tube ...    5.8  \n",
       "77036   The hotel was in a good location but the room...    8.3  \n",
       "39941   The food at breakfast was really good a lot o...    9.6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(PATH_TO_TRAIN_DATA)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpLk8dXBXHaP"
   },
   "source": [
    "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç —Å–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏.\n",
    "–°–¥–µ–ª–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤: —É–¥–∞–ª–∏–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –ø—Ä–∏–≤–µ–¥–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É. \n",
    "–û–¥–Ω–∞–∫–æ –º–æ–∂–Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—Ç—å—Å—è —ç—Ç–∏–º –Ω–∞–±–æ—Ä–æ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π. –ü–æ–¥—É–º–∞–π—Ç–µ, —á—Ç–æ –µ—â–µ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Å —Ç–µ–∫—Å—Ç–∞–º–∏, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –±—É–¥—É—â–∏–º –º–æ–¥–µ–ª—è–º? –î–æ–±–∞–≤—å—Ç–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã –ø–æ–º–æ—á—å –ø–æ –≤–∞—à–µ–º—É –º–Ω–µ–Ω–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['positive'] == ' ']\n",
    "# –ò–º–µ–µ–º —Å–º—ã—Å–ª —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ p/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfkhII5AXHaP"
   },
   "source": [
    "–¢–∞–∫–∂–µ –º—ã –¥–æ–±–∞–≤–∏–ª–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ç–æ–∫–µ–Ω—ã. –¢–µ–ø–µ—Ä—å –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞-—Ä–µ–≤—å—é —Å—Ç–∞–ª–∞ –º–∞—Å—Å–∏–≤–æ–º —Ç–æ–∫–µ–Ω–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tv-gbEKGXHaQ"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def process_text(text):\n",
    "    return [word for word in word_tokenize(text.lower()) if word not in string.punctuation]\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def process_lower(text):\n",
    "    return text.lower()\n",
    "#     return lemmatizer.lemmatize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'now bat leaving their tree they joining call seven satanic hell preacher heading for hall bringing blood of newborn child got succeed if not it satan fall'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Now bats are leaving their trees, They're joining the call, Seven Satanic Hell Preachers Heading for the hall. \\\n",
    "Bringing a blood of a newborn child, Got to succeed, if not it's Satan's fall\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# doc = nlp(s)\n",
    "# print(\" \".join([token.lemma_ for token in doc]))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    " \n",
    "# stop_words = set(stopwords.words('english') + [ \"'s\", \"'re\"]) - {'no'}\n",
    "stop_words = set(['a', 'an', 'the', \"'s\", 'to', \"'re\", 'is', 'are', 'be', 'been', 'was', 'were', 'has', 'had', 'have'])\n",
    "def process_text_advanced(text):\n",
    "    text = [lemmatizer.lemmatize(word) for word in word_tokenize(text.lower()) \n",
    "            if word not in string.punctuation and word not in stop_words]\n",
    "    return ' '.join(text)\n",
    "process_text_advanced(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-X1bXhROXHaQ"
   },
   "outputs": [],
   "source": [
    "# df['negative'] = df['negative'].apply(process_text)\n",
    "# df['positive'] = df['positive'].apply(process_text)\n",
    "\n",
    "df['negative'] = df['negative'].apply(process_text_advanced)\n",
    "df['positive'] = df['positive'].apply(process_text_advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MewBIvp9XHaQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, random_state=1412) # <- –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "y_train = df_train['score'].to_numpy()\n",
    "y_test = df_test['score'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gu1EIc3XHaR"
   },
   "source": [
    "### –ß–∞—Å—Ç—å 1. 1 –±–∞–ª–ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM7ZD9gyXHaR"
   },
   "source": [
    "–û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∏–ª–∏ –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ TF-IDF –≤–µ–∫—Ç–æ—Ä–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2x4yCjh8XHaR"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CufFcfHXhuo"
   },
   "source": [
    "–ü—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ —ç—Ç–æ–π –º–æ–¥–µ–ª—å—é —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ [—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è](https://www.kaggle.com/t/3e8fa6cec6d048bf8e93fb72e441d88c) –∏ —Å–¥–µ–ª–∞–π—Ç–µ —Å–∞–±–º–∏—Ç. –ö–∞–∫–æ–π —É –≤–∞—Å –ø–æ–ª—É—á–∏–ª—Å—è —Å–∫–æ—Ä? –ü—Ä–∏–∫—Ä–µ–ø–∏—Ç–µ —Å–∫—Ä–∏–Ω—à–æ—Ç –∏–∑ –∫—ç–≥–≥–ª–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = (df_train['negative'] + ' ' + df_train['positive']).tolist()\n",
    "# data_test = (df_test['negative'] + ' ' + df_test['positive']).tolist()\n",
    "# –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –º–Ω–µ –ø–æ—Å–æ–≤–µ—Ç–æ–≤–∞–ª–∏ –¥–µ–ª–∞—Ç—å —Ç–∞–∫:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdsp(data):\n",
    "    return pd.DataFrame.sparse.from_spmatrix(data)\n",
    "\n",
    "def transform_fragments(v1, v2, data: list, mode=1):\n",
    "    '''\n",
    "        mode = 1: fitting + transform, input - train / +test data\n",
    "        mode = 2: transform, input - only train or test data\n",
    "    '''\n",
    "    if mode == 2:\n",
    "        assert (len(data) == 1)\n",
    "    \n",
    "    return_data = []\n",
    "    for ind, d in enumerate(data):\n",
    "        if ind > 0 or mode == 2:\n",
    "            data_pos = v1.transform(d['positive'])\n",
    "            data_neg = v2.transform(d['negative'])\n",
    "        else:\n",
    "            data_pos = v1.fit_transform(d['positive'])\n",
    "            data_neg = v2.fit_transform(d['negative'])\n",
    "            \n",
    "        data_ = pd.concat([pdsp(data_pos), pdsp(data_neg)], axis=1, ignore_index=True)\n",
    "        return_data += [data_.sparse.to_coo().tocsr()]\n",
    "    \n",
    "    return return_data, v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1, vec2 = TfidfVectorizer(), TfidfVectorizer()\n",
    "X, vec1, vec2 = transform_fragments(vec1, vec2, [df_train, df_test])\n",
    "X_train, X_test = X[0], X[1]\n",
    "assert X_train.shape[1] == X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í—ã–¥–∞–≤–∞–ª–æ –æ—à–∏–±–∫—É, –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–µ–≤–µ–ª –≤ 100-–±–∞–ª–ª—å–Ω—É—é\n",
    "logreg = LogisticRegression(n_jobs=-1).fit(X_train, (y_train * 10).astype(int))\n",
    "rig = Ridge().fit(X_train, y_train)\n",
    "lasso = Lasso().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Ridge: 0.8316101258538141\n",
      "MAE for Lasso: 1.3166713036799997\n",
      "MAE for LogReg: 0.90306\n"
     ]
    }
   ],
   "source": [
    "y_pred_rig = rig.predict(X_test)\n",
    "y_pred_lass = lasso.predict(X_test)\n",
    "y_pred_lr = logreg.predict(X_test)\n",
    "\n",
    "'''   \n",
    "    MAE for Ridge: 0.8440459199291432\n",
    "    MAE for Lasso: 1.3166713036799997\n",
    "    MAE for LogReg: 0.917024\n",
    "'''\n",
    "\n",
    "print('MAE for Ridge:', MAE(y_test, y_pred_rig))\n",
    "print('MAE for Lasso:', MAE(y_test, y_pred_lass))\n",
    "print('MAE for LogReg:', MAE(y_test, y_pred_lr / 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ridge`-—Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å –ª—É—á—à–µ –≤—Å–µ—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEST_DATA = 'data/test.csv'\n",
    "for_submit_df = pd.read_csv(PATH_TO_TEST_DATA)\n",
    "for_submit_df['negative'] = for_submit_df['negative'].apply(process_text_advanced)\n",
    "for_submit_df['positive'] = for_submit_df['positive'].apply(process_text_advanced)\n",
    "\n",
    "X_subm, _, _ = transform_fragments(vec1, vec2, [for_submit_df], mode=2)\n",
    "\n",
    "y_pred_rig_subm = rig.predict(X_subm[0])\n",
    "submit = for_submit_df.drop(columns=['negative', 'positive'])\n",
    "submit['score'] = y_pred_rig_subm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('sumbit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-4Zve40XHaS"
   },
   "source": [
    "### –ß–∞—Å—Ç—å 2. 2 –±–∞–ª–ª–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYFL-5yFXHaS"
   },
   "source": [
    "–û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∏–ª–∏ –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã—Ö Word2Vec –≤–µ–∫—Ç–æ—Ä–∞—Ö. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jpcCEhBDXHaS"
   },
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_new = df_train.copy()\n",
    "df_test_new = df_test.copy()\n",
    "df_train_new['negative'] = df_train_new['negative'].apply(process_text)\n",
    "df_train_new['positive'] = df_train_new['positive'].apply(process_text)\n",
    "df_test_new['negative'] = df_test_new['negative'].apply(process_text)\n",
    "df_test_new['positive'] = df_test_new['positive'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_pos = Word2Vec(df_train_new['positive'])\n",
    "w2v_model_neg = Word2Vec(df_train_new['negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "# –ü–æ–∑–∞–∏–º—Å—Ç–æ–≤–∞–Ω–æ –∏–∑ https://habr.com/ru/company/ods/blog/329410/\n",
    "class mean_vectorizer():\n",
    "    def __init__(self, w2v_model):\n",
    "        self.w2v_model = w2v_model\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.w2v_dict = dict(zip(self.w2v_model.wv.index_to_key, self.w2v_model.wv.vectors))\n",
    "        self.dim = self.w2v_model.wv.vectors.shape[1]\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return csr_matrix([\n",
    "            np.mean([self.w2v_dict[w] for w in words if w in self.w2v_dict] \n",
    "                or [np.zeros(self.dim)], axis=0) for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self = self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1, vec2 = mean_vectorizer(w2v_model_pos), mean_vectorizer(w2v_model_neg)\n",
    "X, vec1, vec2 = transform_fragments(vec1, vec2, [df_train_new, df_test_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quality(X_train, y_train, X_test, y_test, alert=False):\n",
    "    rig = Ridge().fit(X_train, y_train)\n",
    "    y_pred_rig = rig.predict(X_test)\n",
    "    q_rig = MAE(y_test, y_pred_rig)\n",
    "    \n",
    "    if alert:\n",
    "        print('MAE for Ridge:', q_rig)\n",
    "        \n",
    "    return q_rig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Ridge: 0.9610207923484096\n"
     ]
    }
   ],
   "source": [
    "_ = get_quality(X[0], y_train, X[1], y_test, alert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWrIciGxXHaS"
   },
   "source": [
    "–£—Å—Ä–µ–¥–Ω—è—è w2v –≤–µ–∫—Ç–æ—Ä–∞, –º—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–º–µ–µ—Ç —Ä–∞–≤–Ω–æ—Ü–µ–Ω–Ω—ã–π –≤–∫–ª–∞–¥ –≤ —Å–º—ã—Å–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –æ–¥–Ω–∞–∫–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Å–æ–≤—Å–µ–º —Ç–∞–∫. –¢–µ–ø–µ—Ä—å –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥—Ä—É–≥–æ–π –∫–æ–Ω—Ü–µ–ø—Ü–∏–µ–π –∏ –ø–µ—Ä–µ–≤–∑–≤–µ—Å–∏—Ç—å —Å–ª–æ–≤–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞. –í –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ IDF (Inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class tfidf_vectorizer():\n",
    "    def __init__(self, w2v_model):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.word2weight = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.w2v_dict = dict(zip(self.w2v_model.wv.index_to_key, self.w2v_model.wv.vectors))\n",
    "        self.dim = self.w2v_model.wv.vectors.shape[1]\n",
    "        \n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf = tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return csr_matrix([\n",
    "                np.mean([self.w2v_dict[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.w2v_dict] or\n",
    "                        [np.zeros(self.dim)], axis=0) for words in X\n",
    "            ])\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self = self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mQSuuLP9XHaS"
   },
   "outputs": [],
   "source": [
    "vec1, vec2 = tfidf_vectorizer(w2v_model_pos), tfidf_vectorizer(w2v_model_neg)\n",
    "X, vec1, vec2 = transform_fragments(vec1, vec2, [df_train_new, df_test_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Ridge: 0.9642303949205574\n"
     ]
    }
   ],
   "source": [
    "_ = get_quality(X[0], y_train, X[1], y_test, alert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s-6HQo0XHaT"
   },
   "source": [
    "–ü—Ä–æ–≤–µ–¥–∏—Ç–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ –¥–≤—É—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5728113cf8ff4a9fa20fb6a17c9556eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_dims = [i * 50 for i in range(4, 11)]\n",
    "results = {\n",
    "    'mean': [],\n",
    "    'tfidf': []\n",
    "}\n",
    "\n",
    "for dim in tqdm(emb_dims):\n",
    "    \n",
    "    w2v_model_pos = Word2Vec(df_train_new['positive'], window=30, vector_size=dim)\n",
    "    w2v_model_neg = Word2Vec(df_train_new['negative'], window=30, vector_size=dim)\n",
    "    \n",
    "    vec1, vec2 = mean_vectorizer(w2v_model_pos), mean_vectorizer(w2v_model_neg)\n",
    "    X_mean, _, _ = transform_fragments(vec1, vec2, [df_train_new, df_test_new])\n",
    "    mv_q_rig = get_quality(X_mean[0], y_train, X_mean[1], y_test)\n",
    "    \n",
    "    vec1, vec2 = tfidf_vectorizer(w2v_model_pos), tfidf_vectorizer(w2v_model_neg)\n",
    "    X_idf, _, _ = transform_fragments(vec1, vec2, [df_train_new, df_test_new])\n",
    "    tfidf_q_rig = get_quality(X_idf[0], y_train, X_idf[1], y_test)\n",
    "    \n",
    "    results['mean'].append(mv_q_rig)\n",
    "    results['tfidf'].append(tfidf_q_rig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n",
    "ax1.plot(emb_dims, results['mean'], color='lime', label='mean')\n",
    "ax1.plot(emb_dims, results['tfidf'], color='blue', label='IDF mean')\n",
    "ax1.set_xlabel('embegging size')\n",
    "ax1.set_ylabel('MAE')\n",
    "ax1.set_title('Embegging size - MAE')\n",
    "ax1.legend(shadow=False, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['tfidf'][np.argmin(results['tfidf'])], emb_dims[np.argmin(results['tfidf'])], \\\n",
    "results['mean'][np.argmin(results['mean'])], emb_dims[np.argmin(results['mean'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –°–¥–µ–ª–∞–π—Ç–µ –≤—ã–≤–æ–¥—ã:\n",
    "–ü–æ–Ω–∞—á–∞–ª—É —Å —Ä–æ—Å—Ç–æ–º —Ä–∞–∑–º–µ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–æ —É–ª—É—á—à–∞–µ—Ç—Å—è, –∑–∞—Ç–µ–º –Ω–∞—á–∏–Ω–∞–µ—Ç —Å–∫–∞–∫–∞—Ç—å –∏ –≤–ø–æ–ª–Ω–µ –º–æ–∂–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å—Å—è –∏ –¥–∞–ª—å—à–µ. –≠—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ù–æ –≤ —Ü–µ–ª–æ–º –ø–æ –º–æ–∏–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è–º —Ä–∞–∑–º–µ—Ä –æ—Ç 300 –¥–æ 500 –∫–∞–∂–µ—Ç—Å—è –≤–ø–æ–ª–Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f29vizrmXHaT"
   },
   "source": [
    "–¢–µ–ø–µ—Ä—å –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∏—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∏–ª–∏ –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 300 –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "model_ft_pos = FastText(df_train_new['positive'], window=30, vector_size=300)\n",
    "model_ft_neg = FastText(df_train_new['negative'], window=30, vector_size=300)\n",
    "\n",
    "documents_pos = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_train_new['positive'])]\n",
    "documents_neg = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_train_new['negative'])]\n",
    "model_d2v_pos = Doc2Vec(documents_pos, window=30, vector_size=300)\n",
    "model_d2v_neg = Doc2Vec(documents_neg, window=30, vector_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1, vec2 = tfidf_vectorizer(model_ft_pos), tfidf_vectorizer(model_ft_neg)\n",
    "X, _, _ = transform_fragments(vec1, vec2, [df_train_new, df_test_new])\n",
    "_ = get_quality(X[0], y_train, X[1], y_test, alert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1, vec2 = tfidf_vectorizer(model_d2v_pos), tfidf_vectorizer(model_d2v_neg)\n",
    "X, _, _ = transform_fragments(vec1, vec2, [df_train_new, df_test_new])\n",
    "_ = get_quality(X[0], y_train, X[1], y_test, alert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –í—ã–≤–æ–¥—ã:\n",
    "–ù–µ –ª—É—á—à–µ, —á–µ–º word2vec + Ridge + IGF-–≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ. –ï—Å–ª–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —ç—Ç–∏ –¥–≤–∞ –º–µ—Ç–æ–¥–∞, —Ç–æ –∏–∑ –Ω–∏—Ö –ª—É—á—à–µ —Å–µ–±—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç IDF-–≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ + Ridge + doc2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AjabHMsXXBu"
   },
   "source": [
    "–ü—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ –≤–∞—à–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é –∏–∑ —ç—Ç–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ [—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è](https://www.kaggle.com/t/3e8fa6cec6d048bf8e93fb72e441d88c) –∏ —Å–¥–µ–ª–∞–π—Ç–µ —Å–∞–±–º–∏—Ç. –ö–∞–∫–æ–π —É –≤–∞—Å –ø–æ–ª—É—á–∏–ª—Å—è —Å–∫–æ—Ä? –ü—Ä–∏–∫—Ä–µ–ø–∏—Ç–µ —Å–∫—Ä–∏–Ω—à–æ—Ç –∏–∑ –∫—ç–≥–≥–ª–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = tfidf_vectorizer(model_d2v)\n",
    "X_train_all = vectorizer.fit_transform(df_train_all)\n",
    "X_subm = vectorizer.transform(subm)\n",
    "\n",
    "model = Ridge().fit(X_train_all, df['score'].to_numpy())\n",
    "y_pred_rig_subm = model.predict(X_subm)\n",
    "submit = for_submit_df.drop(columns=['negative', 'positive'])\n",
    "submit['score'] = y_pred_rig_subm\n",
    "\n",
    "submit.to_csv('sumbit_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO5TZriLXHaT"
   },
   "source": [
    "### –ß–∞—Å—Ç—å 3. 4 –±–∞–ª–ª–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RNngNdWXHaT"
   },
   "source": [
    "–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –ø—Ä–æ—Ö–æ–¥–∏–ª–∏ –≤ –Ω–∞—à–µ–º –∫—É—Ä—Å–µ. –û–±—É—á–∏—Ç–µ RNN/Transformer –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8YdTedQXHaT"
   },
   "source": [
    "–ï—Å–ª–∏ –±—É–¥–µ—Ç–µ –æ–±—É—á–∞—Ç—å RNN, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–∑—ã–≤—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö.\n",
    "\n",
    "–ß—Ç–æ–±—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è DataLoader, –≤—Å–µ –µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏. –î–ª—è —ç—Ç–æ–≥–æ –≤—ã –º–æ–∂–µ—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –Ω—É–ª–µ–≤–æ–π –ø–∞–¥–¥–∏–Ω–≥ –∫–æ –≤—Å–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (—Å–º –ø—Ä–∏–º–µ—Ä pad_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = (df_train['negative'] + ' ' + df_train['positive']).tolist()\n",
    "# data_test = (df_test['negative'] + ' ' + df_test['positive']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89Y9wsViXHaU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPOjSwwwXHaU"
   },
   "outputs": [],
   "source": [
    "WORDS = set()\n",
    "for sent in list(df['positive']):\n",
    "    for w in sent:\n",
    "        WORDS.add(w)\n",
    "        \n",
    "for sent in list(df['negative']):\n",
    "    for w in sent:\n",
    "        WORDS.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMQ-cVxGXHaU"
   },
   "outputs": [],
   "source": [
    "int2word = dict(enumerate(tuple(WORDS)))\n",
    "word2int = {w: ii for ii, w in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiKyqKIWXHaU"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = max(max(df['positive'].apply(len)), max(df['negative'].apply(len)))\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81o6S3_AXHaU",
    "outputId": "6ac4d30d-efc7-4324-eeda-d2950734813f"
   },
   "source": [
    "### –°–ª–µ–¥—É—é—â–∏–π –∫–æ–¥ –±—É–¥–µ—Ç –≤–æ –º–Ω–æ–≥–æ–º –ø–æ–∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω —Å —Å–µ–º–∏–Ω–∞—Ä–∞ –≤–æ –∏–∑–±–µ–∂–∞–Ω–∏–µ –≥–æ–ª–æ–≤–Ω–æ–π –±–æ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def dataset_iterator(texts):\n",
    "    for text in texts:\n",
    "        yield text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(\n",
    "    dataset_iterator(data_train),\n",
    "                                    # –≥/–ø\n",
    "    specials=['<pad>', '<unk>'], min_freq=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(vocab, data):\n",
    "    tokens = []\n",
    "    for text in dataset_iterator(data):\n",
    "        sentence_tokens = [vocab[word] if word in vocab else vocab['<unk>'] for word in text]\n",
    "        tokens += [sentence_tokens]\n",
    "    return tokens\n",
    "\n",
    "train_tokens = tokenize_sentences(vocab, data_train)\n",
    "test_tokens = tokenize_sentences(vocab, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "lengths = np.array([len(tokens) for tokens in train_tokens])\n",
    "sns.displot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢—É—Ç –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã —É–¥–∞–ª–∏—Ç—å —Ç–µ –æ—Ç–∑—ã–≤—ã, –≥–¥–µ –≤ –ø–æ–ª—è—Ö –ø—É—Å—Ç–æ, –∏–ª–∏ –∂–µ —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è–º–∏ –∞-–ª—è `no positive`/`no negative`. –õ—é–¥–∏ —Å–∫–ª–æ–Ω–Ω—ã –Ω–∏—á–µ–≥–æ –Ω–µ –ø–∏—Å–∞—Ç—å –∏ —Å—Ç–∞–≤–∏—Ç—å –≤—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏, –ª–∏–±–æ –∂–µ –ø–∏—Å–∞—Ç—å `no positive` –∏ —Å—Ç–∞–≤–∏—Ç—å 0 –∏–ª–∏, –Ω–∞–ø—Ä–æ—Ç–∏–≤, –ø–∏—Å–∞—Ç—å `no negative` –∏ —Å—Ç–∞–≤–∏—Ç—å 10. –ü–æ—ç—Ç–æ–º—É —è —Ä–µ—à–∏–ª –Ω–∏—á–µ–≥–æ –Ω–µ —É–¥–∞–ª—è—Ç—å - —Ç–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç —Å–∏–ª—å–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 400\n",
    "def get_data(data):\n",
    "    tokenized_data = torch.full((len(data), max_length), vocab['<pad>'], dtype=torch.int32)\n",
    "    for i, tokens in enumerate(data):\n",
    "        length = min(max_length, len(tokens))\n",
    "        tokenized_data[i, :length] = torch.tensor(tokens[:length])\n",
    "    return tokenized_data\n",
    "    \n",
    "tokenized_train = get_data(train_tokens)\n",
    "tokenized_test = get_data(test_tokens)\n",
    "    \n",
    "targets_train = torch.tensor(y_train, dtype=torch.int32)\n",
    "targets_test = torch.tensor(y_test, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(tokenized_train, targets_train)\n",
    "test_dataset = TensorDataset(tokenized_test, targets_test)\n",
    "\n",
    "# –≥/–ø\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQI2EGzbXHaV"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "train_pos_pad = pad_sequence([torch.as_tensor([word2int[w] for w in seq][:MAX_LEN]) for seq in df_train['positive']], \n",
    "                           batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76PDJ4yTXHaV"
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        ## TODO\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        ## TODO\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ## TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8jxX9B2XHaV"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataset = ReviewsDataset(df_train)\n",
    "test_dataset = ReviewsDataset(df_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f6EmAuJXHaV"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "for n in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    ## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3OeNQkoXHaW"
   },
   "source": [
    "### –ö–æ–Ω—Ç–µ—Å—Ç (–¥–æ 3 –±–∞–ª–ª–æ–≤)\n",
    "\n",
    "–ü–æ –∏—Ç–æ–≥–∞–º –≤—Å–µ—Ö –≤–∞—à–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é —Å—á–∏—Ç–∞–µ—Ç–µ –ª—É—á—à–µ–π. –°–¥–µ–ª–∞–π—Ç–µ —Å–∞–±–º–∏—Ç –≤ –∫–æ–Ω—Ç–µ—Å—Ç. –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—à–µ–≥–æ —Å–∫–æ—Ä–∞ –Ω–∞ –ø—É–±–ª–∏—á–Ω–æ–º –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ, –º—ã –Ω–∞—á–∏—Å–ª–∏–º –≤–∞–º –±–∞–ª–ª—ã:\n",
    "\n",
    " - <0.76 - 3 –±–∞–ª–ª–∞\n",
    " - [0.76; 0.78) - 2 –±–∞–ª–ª–∞\n",
    " - [0.78; 0.8) - 1 –±–∞–ª–ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfORFaucXHaW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_kaggle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
