{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "notebookf3a6fa3880 (2).ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5eddabf98c18417b81e22ebf6a330cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6aaa62a5c1b042e6887deba22d8bfb75",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_909acff990ca4b958f9b852f32310f80",
              "IPY_MODEL_41a484bbeac84f4cb6bc2ebb9a0dc72c",
              "IPY_MODEL_0f00f02a2f3248638ff87a1c28f51bc9"
            ]
          }
        },
        "6aaa62a5c1b042e6887deba22d8bfb75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "909acff990ca4b958f9b852f32310f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_187d454ecbf849b8b68bc4cc7088e393",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_079375f382b4401f821766e6f50333b7"
          }
        },
        "41a484bbeac84f4cb6bc2ebb9a0dc72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_69976f23fa994e2bbdc9e0114eb3fca4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e17bec2e0772435b82ae8fd173e62271"
          }
        },
        "0f00f02a2f3248638ff87a1c28f51bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_257de7cab68849689317b27a77440e6f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [17:19&lt;00:00, 224.14s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d314dd9a7ae942fabea043896892f754"
          }
        },
        "187d454ecbf849b8b68bc4cc7088e393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "079375f382b4401f821766e6f50333b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69976f23fa994e2bbdc9e0114eb3fca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e17bec2e0772435b82ae8fd173e62271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "257de7cab68849689317b27a77440e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d314dd9a7ae942fabea043896892f754": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Введение в глубинное обучение, ФКН ВШЭ\n",
        "\n",
        "## Домашнее задание 3. Обработка текстов.\n",
        "\n",
        "### Общая информация\n",
        "\n",
        "Дата выдачи: 13.01.2022\n",
        "\n",
        "Мягкий дедлайн: 23:59MSK 6.02.2022\n",
        "\n",
        "Жесткий дедлайн: 23:59MSK 10.02.2022\n",
        "\n",
        "Оценка после штрафа после мягкого дедлайна вычисляется по формуле $M_{penalty} = M_{full} \\cdot 0.85^{t/1440}$, где $M_{full}$ — полная оценка за работу без учета штрафа, а $t$ — время в минутах, прошедшее после мягкого дедлайна (округление до двух цифр после запятой). Таким образом, спустя первые сутки после мягкого дедлайна вы не можете получить оценку выше 8.5, а если сдать перед самым жестким дедлайном, то ваш максимум — 5.22 балла.\n",
        "\n",
        "### Оценивание и штрафы\n",
        "\n",
        "Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после указанного срока сдачи нельзя.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
        "\n",
        "### О задании\n",
        "\n",
        "В данном домашнем задании вам предстоит предсказывать пользовательскую оценку отеля по тексту отзыва. Нужно обучиться на данных с кэггла и заслать в [соревнование](https://www.kaggle.com/t/3e8fa6cec6d048bf8e93fb72e441d88c) предикт. По той же ссылке можете скачать данные.\n",
        "\n",
        "Мы собрали для вас отзывы по 1500 отелям из совершенно разных уголков мира. Что это за отели - секрет. Вам дан текст отзыва и пользовательская оценка отеля. Ваша задача - научиться предсказывать оценку отеля по отзыву.\n",
        "\n",
        "Главная метрика - Mean Absolute Error (MAE). Во всех частях домашней работы вам нужно получить значение MAE не превышающее 0.92 на публичном лидерборде. В противном случае мы будем вынуждены не засчитать задание :( \n",
        "\n",
        "#### Про данные:\n",
        "Каждое ревью состоит из двух текстов: positive и negative - плюсы и минусы отеля. В столбце score находится оценка пользователя - вещественное число 0 до 10. Вам нужно извлечь признаки из этих текстов и предсказать по ним оценку.\n",
        "\n",
        "Для локального тестирования используйте предоставленное разбиение на трейн и тест.\n",
        "\n",
        "Good luck & have fun! 💪"
      ],
      "metadata": {
        "id": "OCfOAvvpXHaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:54:03.576909Z",
          "iopub.execute_input": "2022-02-04T18:54:03.577436Z",
          "iopub.status.idle": "2022-02-04T18:54:05.145344Z",
          "shell.execute_reply.started": "2022-02-04T18:54:03.577346Z",
          "shell.execute_reply": "2022-02-04T18:54:05.144266Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9b67bjwvfAE",
        "outputId": "fa69537b-2c4d-4668-8601-7268b6471726"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.2+cu102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Использовать любые данные для обучения кроме предоставленных организаторами строго запрещено. В последней части можно использовать предобученные модели из библиотеки `transformers`."
      ],
      "metadata": {
        "id": "T6Ej16t1XHaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_TRAIN_DATA = 'drive/MyDrive/data/train.csv'"
      ],
      "metadata": {
        "id": "g96zAM-fwSJS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(PATH_TO_TRAIN_DATA)\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "6kJRM6ZUXHaO",
        "outputId": "b624a839-518b-4456-dfcf-eaf2bdedf6cd",
        "execution": {
          "iopub.status.busy": "2022-02-04T18:54:05.156238Z",
          "iopub.execute_input": "2022-02-04T18:54:05.157252Z",
          "iopub.status.idle": "2022-02-04T18:54:06.054300Z",
          "shell.execute_reply.started": "2022-02-04T18:54:05.157193Z",
          "shell.execute_reply": "2022-02-04T18:54:06.053460Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-68489965-7770-4cdb-9b30-bfb48c3a5145\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2050</th>\n",
              "      <td>0577cd13e8245a98e0635e0f7f4ff5ee</td>\n",
              "      <td>Had a slight problem with our meal at the in ...</td>\n",
              "      <td>Reception area very beautiful and Made a grea...</td>\n",
              "      <td>9.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31907</th>\n",
              "      <td>520bcdce8faf17e9544cb337d8051138</td>\n",
              "      <td>Carpeted floor aged bathroom cobwebs at the c...</td>\n",
              "      <td>The location and the breakfast is great espec...</td>\n",
              "      <td>6.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90725</th>\n",
              "      <td>e82d1266336ae99076f306a6c451d9b6</td>\n",
              "      <td>11 steps down from roadway to reception No as...</td>\n",
              "      <td>Location to shops and to Metro line</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99321</th>\n",
              "      <td>fe3d60970724ac44f25a25373e48d861</td>\n",
              "      <td>overpriced</td>\n",
              "      <td>Good location very close to tram stop direct ...</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31526</th>\n",
              "      <td>512424ee53f417ce5525a31054d0bbbe</td>\n",
              "      <td>It would have been nice to know that in this ...</td>\n",
              "      <td>It was good it was near the central line just...</td>\n",
              "      <td>8.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35809</th>\n",
              "      <td>5c119f8f25ba0b347b9a944d80317b03</td>\n",
              "      <td>Bad greeting highly overpriced and window bli...</td>\n",
              "      <td>Nothing</td>\n",
              "      <td>4.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75472</th>\n",
              "      <td>c1632e67adf46ab9c0c84f730cd0b2eb</td>\n",
              "      <td>Poor room for the Hilton</td>\n",
              "      <td>No Positive</td>\n",
              "      <td>7.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15688</th>\n",
              "      <td>28a9061e07f487c9fafe2823e6b32e83</td>\n",
              "      <td>My booking was described as one for a deluxe ...</td>\n",
              "      <td>No traffic noise but doors banging elsewhere ...</td>\n",
              "      <td>6.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8858</th>\n",
              "      <td>17173c60b8bd669e729afbcbc7afd84b</td>\n",
              "      <td>Rooms are so small and when we asked the staf...</td>\n",
              "      <td>Location was good</td>\n",
              "      <td>2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76494</th>\n",
              "      <td>c3df5c647b22ea84c89dd5cf2be82b77</td>\n",
              "      <td>Finishes not the best</td>\n",
              "      <td>Handy location nice staff innovative room layout</td>\n",
              "      <td>8.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68489965-7770-4cdb-9b30-bfb48c3a5145')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-68489965-7770-4cdb-9b30-bfb48c3a5145 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-68489965-7770-4cdb-9b30-bfb48c3a5145');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                              review_id  ... score\n",
              "2050   0577cd13e8245a98e0635e0f7f4ff5ee  ...   9.2\n",
              "31907  520bcdce8faf17e9544cb337d8051138  ...   6.3\n",
              "90725  e82d1266336ae99076f306a6c451d9b6  ...   7.1\n",
              "99321  fe3d60970724ac44f25a25373e48d861  ...   7.5\n",
              "31526  512424ee53f417ce5525a31054d0bbbe  ...   8.8\n",
              "35809  5c119f8f25ba0b347b9a944d80317b03  ...   4.2\n",
              "75472  c1632e67adf46ab9c0c84f730cd0b2eb  ...   7.1\n",
              "15688  28a9061e07f487c9fafe2823e6b32e83  ...   6.3\n",
              "8858   17173c60b8bd669e729afbcbc7afd84b  ...   2.9\n",
              "76494  c3df5c647b22ea84c89dd5cf2be82b77  ...   8.8\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработка текста может сказываться на качестве вашей модели.\n",
        "Сделаем небольшой препроцессинг текстов: удалим знаки препинания, приведем все слова к нижнему регистру. \n",
        "Однако можно не ограничиваться этим набором преобразований. Подумайте, что еще можно сделать с текстами, чтобы помочь будущим моделям? Добавьте преобразования, которые могли бы помочь по вашему мнению."
      ],
      "metadata": {
        "id": "bpLk8dXBXHaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также мы добавили разбиение текстов на токены. Теперь каждая строка-ревью стала массивом токенов."
      ],
      "metadata": {
        "id": "SfkhII5AXHaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L3zVmsu0ymI",
        "outputId": "eb6ab5a5-8b0c-41dd-e111-6b44e474a350"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text):\n",
        "    return [word for word in word_tokenize(text.lower()) if word not in string.punctuation]\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "def process_lower(text):\n",
        "    return text.lower()\n",
        "#     return lemmatizer.lemmatize(text.lower())"
      ],
      "metadata": {
        "id": "tv-gbEKGXHaQ",
        "execution": {
          "iopub.status.busy": "2022-02-04T18:54:06.063045Z",
          "iopub.execute_input": "2022-02-04T18:54:06.063307Z",
          "iopub.status.idle": "2022-02-04T18:54:07.782030Z",
          "shell.execute_reply.started": "2022-02-04T18:54:06.063276Z",
          "shell.execute_reply": "2022-02-04T18:54:07.781268Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Now bats are leaving their trees, They're joining the call, Seven Satanic Hell Preachers Heading for the hall. \\\n",
        "Bringing a blood of a newborn child, Got to succeed, if not it's Satan's fall\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "stop_words = set(stopwords.words('english') + [ \"'s\", \"'re\"]) - {'no'}\n",
        "# stop_words = set(['a', 'an', 'the', \"'s\", 'to', \"'re\", 'is', 'are', 'be', 'been', 'was', 'were', 'has', 'had', 'have'])\n",
        "def process_text_advanced(text):\n",
        "    text = [lemmatizer.lemmatize(word) for word in word_tokenize(text.lower()) \n",
        "            if word not in string.punctuation and word not in stop_words]\n",
        "    return ' '.join(text)\n",
        "process_text_advanced(s)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:54:07.783119Z",
          "iopub.execute_input": "2022-02-04T18:54:07.783755Z",
          "iopub.status.idle": "2022-02-04T18:54:10.125380Z",
          "shell.execute_reply.started": "2022-02-04T18:54:07.783712Z",
          "shell.execute_reply": "2022-02-04T18:54:10.124755Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "T2yHjx42vfAK",
        "outputId": "24696e47-b56b-461f-a450-0b657165dbbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bat leaving tree joining call seven satanic hell preacher heading hall bringing blood newborn child got succeed satan fall'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['negative'] = df['negative'].apply(process_text_advanced)\n",
        "df['positive'] = df['positive'].apply(process_text_advanced)"
      ],
      "metadata": {
        "id": "-X1bXhROXHaQ",
        "execution": {
          "iopub.status.busy": "2022-02-04T18:54:10.126666Z",
          "iopub.execute_input": "2022-02-04T18:54:10.127057Z",
          "iopub.status.idle": "2022-02-04T18:55:12.867281Z",
          "shell.execute_reply.started": "2022-02-04T18:54:10.127008Z",
          "shell.execute_reply": "2022-02-04T18:55:12.865969Z"
        },
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, random_state=1412) # <- для локального тестирования\n",
        "y_train = df_train['score'].to_numpy()\n",
        "y_test = df_test['score'].to_numpy()"
      ],
      "metadata": {
        "id": "MewBIvp9XHaQ",
        "execution": {
          "iopub.status.busy": "2022-02-04T18:55:12.868354Z",
          "iopub.execute_input": "2022-02-04T18:55:12.868594Z",
          "iopub.status.idle": "2022-02-04T18:55:12.909594Z",
          "shell.execute_reply.started": "2022-02-04T18:55:12.868546Z",
          "shell.execute_reply": "2022-02-04T18:55:12.908809Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Часть 1. 1 балл"
      ],
      "metadata": {
        "id": "3gu1EIc3XHaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучите логистическую или линейную регрессию на TF-IDF векторах текстов."
      ],
      "metadata": {
        "id": "DM7ZD9gyXHaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_absolute_error as MAE"
      ],
      "metadata": {
        "id": "2x4yCjh8XHaR",
        "execution": {
          "iopub.status.busy": "2022-02-04T18:55:12.910563Z",
          "iopub.execute_input": "2022-02-04T18:55:12.911336Z",
          "iopub.status.idle": "2022-02-04T18:55:12.917700Z",
          "shell.execute_reply.started": "2022-02-04T18:55:12.911295Z",
          "shell.execute_reply": "2022-02-04T18:55:12.916943Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предскажите этой моделью тестовые данные из [соревнования](https://www.kaggle.com/t/3e8fa6cec6d048bf8e93fb72e441d88c) и сделайте сабмит. Какой у вас получился скор? Прикрепите скриншот из кэггла."
      ],
      "metadata": {
        "id": "0CufFcfHXhuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_train = (df_train['negative'] + ' ' + df_train['positive']).tolist()\n",
        "# data_test = (df_test['negative'] + ' ' + df_test['positive']).tolist()\n",
        "# Вместо этого мне посоветовали делать так:"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:55:12.921470Z",
          "iopub.execute_input": "2022-02-04T18:55:12.921786Z",
          "iopub.status.idle": "2022-02-04T18:55:12.931531Z",
          "shell.execute_reply.started": "2022-02-04T18:55:12.921753Z",
          "shell.execute_reply": "2022-02-04T18:55:12.930205Z"
        },
        "trusted": true,
        "id": "pvkLOQfPvfAN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdsp(data):\n",
        "    return pd.DataFrame.sparse.from_spmatrix(data)\n",
        "\n",
        "def transform_fragments(v1, v2, data: list, mode=1):\n",
        "    '''\n",
        "        mode = 1: fitting + transform, input - train / +test data\n",
        "        mode = 2: transform, input - only train or test data\n",
        "    '''\n",
        "    if mode == 2:\n",
        "        assert (len(data) == 1)\n",
        "    \n",
        "    return_data = []\n",
        "    for ind, d in enumerate(data):\n",
        "        if ind > 0 or mode == 2:\n",
        "            data_pos = v1.transform(d['positive'])\n",
        "            data_neg = v2.transform(d['negative'])\n",
        "        else:\n",
        "            data_pos = v1.fit_transform(d['positive'])\n",
        "            data_neg = v2.fit_transform(d['negative'])\n",
        "            \n",
        "        data_ = pd.concat([pdsp(data_pos), pdsp(data_neg)], axis=1, ignore_index=True)\n",
        "        return_data += [data_.sparse.to_coo().tocsr()]\n",
        "    \n",
        "    return return_data, v1, v2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:55:12.933004Z",
          "iopub.execute_input": "2022-02-04T18:55:12.933302Z",
          "iopub.status.idle": "2022-02-04T18:55:12.946270Z",
          "shell.execute_reply.started": "2022-02-04T18:55:12.933263Z",
          "shell.execute_reply": "2022-02-04T18:55:12.945129Z"
        },
        "trusted": true,
        "id": "KcIoP0SVvfAO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec1, vec2 = TfidfVectorizer(), TfidfVectorizer()\n",
        "X, vec1, vec2 = transform_fragments(vec1, vec2, [df_train, df_test])\n",
        "X_train, X_test = X[0], X[1]\n",
        "assert X_train.shape[1] == X_test.shape[1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:55:12.948025Z",
          "iopub.execute_input": "2022-02-04T18:55:12.948621Z",
          "iopub.status.idle": "2022-02-04T18:55:29.694427Z",
          "shell.execute_reply.started": "2022-02-04T18:55:12.948545Z",
          "shell.execute_reply": "2022-02-04T18:55:29.693631Z"
        },
        "trusted": true,
        "id": "N7dip7avvfAO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Выдавало ошибку, поэтому перевел в 100-балльную\n",
        "logreg = LogisticRegression(solver=\"liblinear\").fit(X_train, (y_train * 10).astype(int))\n",
        "rig = Ridge().fit(X_train, y_train)\n",
        "lasso = Lasso().fit(X_train, y_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:55:29.695679Z",
          "iopub.execute_input": "2022-02-04T18:55:29.695921Z",
          "iopub.status.idle": "2022-02-04T18:58:08.642893Z",
          "shell.execute_reply.started": "2022-02-04T18:55:29.695890Z",
          "shell.execute_reply": "2022-02-04T18:58:08.641637Z"
        },
        "trusted": true,
        "id": "LS7XqKYdvfAO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_rig = rig.predict(X_test)\n",
        "y_pred_lass = lasso.predict(X_test)\n",
        "y_pred_lr = logreg.predict(X_test)\n",
        "\n",
        "print('MAE for Ridge:', MAE(y_test, y_pred_rig))\n",
        "print('MAE for Lasso:', MAE(y_test, y_pred_lass))\n",
        "print('MAE for LogReg:', MAE(y_test, y_pred_lr / 10))\n",
        "\n",
        "# MAE for Ridge: 0.8427509194337326\n",
        "# MAE for Lasso: 1.3166713036799997\n",
        "# MAE for LogReg: 0.9546279999999999"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:58:08.644774Z",
          "iopub.execute_input": "2022-02-04T18:58:08.646784Z",
          "iopub.status.idle": "2022-02-04T18:58:08.769162Z",
          "shell.execute_reply.started": "2022-02-04T18:58:08.646717Z",
          "shell.execute_reply": "2022-02-04T18:58:08.768511Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt_wXGUrvfAP",
        "outputId": "01533a41-e3b3-4087-c0ea-7e2690eb5820"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE for Ridge: 0.8427509194337326\n",
            "MAE for Lasso: 1.3166713036799997\n",
            "MAE for LogReg: 0.9546279999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Ridge`-регрессия справилась лучше всех."
      ],
      "metadata": {
        "id": "aIBwKt0uvfAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_TEST_DATA = 'drive/MyDrive/data/test.csv'\n",
        "for_submit_df = pd.read_csv(PATH_TO_TEST_DATA)\n",
        "for_submit_df['negative'] = for_submit_df['negative'].apply(process_text_advanced)\n",
        "for_submit_df['positive'] = for_submit_df['positive'].apply(process_text_advanced)\n",
        "\n",
        "X_subm, _, _ = transform_fragments(vec1, vec2, [for_submit_df], mode=2)\n",
        "\n",
        "y_pred_rig_subm = rig.predict(X_subm[0])\n",
        "for_submit_df.drop(columns=['negative', 'positive'], inplace=True)\n",
        "for_submit_df['score'] = y_pred_rig_subm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:58:08.770364Z",
          "iopub.execute_input": "2022-02-04T18:58:08.770784Z",
          "iopub.status.idle": "2022-02-04T18:58:28.810314Z",
          "shell.execute_reply.started": "2022-02-04T18:58:08.770753Z",
          "shell.execute_reply": "2022-02-04T18:58:28.809281Z"
        },
        "trusted": true,
        "id": "fgpD64s2vfAP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for_submit_df.to_csv('sumbit.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:58:28.811711Z",
          "iopub.execute_input": "2022-02-04T18:58:28.812017Z",
          "iopub.status.idle": "2022-02-04T18:58:28.918394Z",
          "shell.execute_reply.started": "2022-02-04T18:58:28.811975Z",
          "shell.execute_reply": "2022-02-04T18:58:28.917535Z"
        },
        "trusted": true,
        "id": "1RBNmuk_vfAP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X, vec1, vec2, X_train, X_test, y_pred_lr, y_pred_lass, y_pred_rig, X_subm, for_submit_df"
      ],
      "metadata": {
        "id": "GksOGVqqAsA2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Часть 2. 2 балла"
      ],
      "metadata": {
        "id": "E-4Zve40XHaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучите логистическую или линейную регрессию на усредненных Word2Vec векторах. "
      ],
      "metadata": {
        "id": "cYFL-5yFXHaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SeqkWXC1dxA",
        "outputId": "8f850bc4-b5c2-48d9-b198-9e724fab2ece"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "jpcCEhBDXHaS",
        "execution": {
          "iopub.status.busy": "2022-02-04T18:58:28.919677Z",
          "iopub.execute_input": "2022-02-04T18:58:28.920477Z",
          "iopub.status.idle": "2022-02-04T18:58:30.148083Z",
          "shell.execute_reply.started": "2022-02-04T18:58:28.920436Z",
          "shell.execute_reply": "2022-02-04T18:58:30.147294Z"
        },
        "trusted": true
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['negative'] = df_train['negative'].apply(process_text)\n",
        "df_train['positive'] = df_train['positive'].apply(process_text)\n",
        "df_test['negative'] = df_test['negative'].apply(process_text)\n",
        "df_test['positive'] = df_test['positive'].apply(process_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:58:30.149790Z",
          "iopub.execute_input": "2022-02-04T18:58:30.150123Z",
          "iopub.status.idle": "2022-02-04T18:59:12.363963Z",
          "shell.execute_reply.started": "2022-02-04T18:58:30.150081Z",
          "shell.execute_reply": "2022-02-04T18:59:12.362563Z"
        },
        "trusted": true,
        "id": "UoK4J98lvfAQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model_pos = Word2Vec(df_train['positive'], window=7, epochs=7)\n",
        "w2v_model_neg = Word2Vec(df_train['negative'], window=7, epochs=7)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:59:12.365419Z",
          "iopub.execute_input": "2022-02-04T18:59:12.365727Z",
          "iopub.status.idle": "2022-02-04T18:59:22.791012Z",
          "shell.execute_reply.started": "2022-02-04T18:59:12.365694Z",
          "shell.execute_reply": "2022-02-04T18:59:22.790124Z"
        },
        "trusted": true,
        "id": "cyaOEzOUvfAQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse.csr import csr_matrix\n",
        "# Позаимстовано из https://habr.com/ru/company/ods/blog/329410/\n",
        "class mean_vectorizer():\n",
        "    def __init__(self, w2v_model):\n",
        "        self.w2v_model = w2v_model\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.w2v_dict = dict(zip(self.w2v_model.wv.index_to_key, self.w2v_model.wv.vectors))\n",
        "        self.dim = self.w2v_model.wv.vectors.shape[1]\n",
        "        return self \n",
        "\n",
        "    def transform(self, X):\n",
        "        return csr_matrix([\n",
        "            np.mean([self.w2v_dict[w] for w in words if w in self.w2v_dict] \n",
        "                or [np.zeros(self.dim)], axis=0) for words in X\n",
        "        ])\n",
        "    \n",
        "    def fit_transform(self, X):\n",
        "        self = self.fit(X)\n",
        "        return self.transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:59:22.792279Z",
          "iopub.execute_input": "2022-02-04T18:59:22.792523Z",
          "iopub.status.idle": "2022-02-04T18:59:22.801984Z",
          "shell.execute_reply.started": "2022-02-04T18:59:22.792494Z",
          "shell.execute_reply": "2022-02-04T18:59:22.801112Z"
        },
        "trusted": true,
        "id": "VNvyvqRcvfAQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec1, vec2 = mean_vectorizer(w2v_model_pos), mean_vectorizer(w2v_model_neg)\n",
        "X, vec1, vec2 = transform_fragments(vec1, vec2, [df_train, df_test])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:59:22.803380Z",
          "iopub.execute_input": "2022-02-04T18:59:22.803645Z",
          "iopub.status.idle": "2022-02-04T18:59:32.513994Z",
          "shell.execute_reply.started": "2022-02-04T18:59:22.803606Z",
          "shell.execute_reply": "2022-02-04T18:59:32.513062Z"
        },
        "trusted": true,
        "id": "qJKfQq8NvfAR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_quality(X_train, y_train, X_test, y_test, alert=False, alpha=1.):\n",
        "    rig = Ridge(alpha=alpha).fit(X_train, y_train)\n",
        "    y_pred_rig = rig.predict(X_test)\n",
        "    y_pred_rig_train = rig.predict(X_train)\n",
        "    q_rig = MAE(y_test, y_pred_rig)\n",
        "    q_rig_train = MAE(y_train, y_pred_rig_train)\n",
        "    \n",
        "    if alert:\n",
        "        print('Test MAE for Ridge:', q_rig)\n",
        "        print('Train MAE for Ridge:', q_rig_train)\n",
        "        \n",
        "    return q_rig"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:59:32.515408Z",
          "iopub.execute_input": "2022-02-04T18:59:32.515642Z",
          "iopub.status.idle": "2022-02-04T18:59:32.521304Z",
          "shell.execute_reply.started": "2022-02-04T18:59:32.515614Z",
          "shell.execute_reply": "2022-02-04T18:59:32.520374Z"
        },
        "trusted": true,
        "id": "EHx4BLeFvfAR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = get_quality(X[0], y_train, X[1], y_test, alert=True)\n",
        "# Test MAE for Ridge: 0.9604130193301033\n",
        "# Train MAE for Ridge: 0.9530745706699796"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:59:32.522772Z",
          "iopub.execute_input": "2022-02-04T18:59:32.523021Z",
          "iopub.status.idle": "2022-02-04T18:59:36.409147Z",
          "shell.execute_reply.started": "2022-02-04T18:59:32.522994Z",
          "shell.execute_reply": "2022-02-04T18:59:36.408001Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgl7pduRvfAR",
        "outputId": "ddf1dc3b-1f36-4799-d8ca-0fac36a43e91"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE for Ridge: 0.9623280538662836\n",
            "Train MAE for Ridge: 0.9542300129739026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Усредняя w2v вектора, мы предполагаем, что каждое слово имеет равноценный вклад в смысл предложения, однако это может быть не совсем так. Теперь попробуйте воспользоваться другой концепцией и перевзвесить слова при получении итогового эмбеддинга текста. В качестве весов используйте IDF (Inverse document frequency)"
      ],
      "metadata": {
        "id": "KWrIciGxXHaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class tfidf_vectorizer():\n",
        "    def __init__(self, w2v_model):\n",
        "        self.w2v_model = w2v_model\n",
        "        self.word2weight = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.w2v_dict = dict(zip(self.w2v_model.wv.index_to_key, self.w2v_model.wv.vectors))\n",
        "        self.dim = self.w2v_model.wv.vectors.shape[1]\n",
        "        \n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf = tfidf.fit(X)\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return csr_matrix([\n",
        "                np.mean([self.w2v_dict[w] * self.word2weight[w]\n",
        "                         for w in words if w in self.w2v_dict] or\n",
        "                        [np.zeros(self.dim)], axis=0) for words in X\n",
        "            ])\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self = self.fit(X)\n",
        "        return self.transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T18:59:36.411093Z",
          "iopub.execute_input": "2022-02-04T18:59:36.411435Z",
          "iopub.status.idle": "2022-02-04T18:59:36.424234Z",
          "shell.execute_reply.started": "2022-02-04T18:59:36.411363Z",
          "shell.execute_reply": "2022-02-04T18:59:36.422925Z"
        },
        "trusted": true,
        "id": "3a4zuSy3vfAR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec1, vec2 = tfidf_vectorizer(w2v_model_pos), tfidf_vectorizer(w2v_model_neg)\n",
        "X, vec1, vec2 = transform_fragments(vec1, vec2, [df_train, df_test])"
      ],
      "metadata": {
        "id": "mQSuuLP9XHaS",
        "execution": {
          "iopub.status.busy": "2022-02-04T18:59:36.425951Z",
          "iopub.execute_input": "2022-02-04T18:59:36.426799Z",
          "iopub.status.idle": "2022-02-04T19:00:03.352153Z",
          "shell.execute_reply.started": "2022-02-04T18:59:36.426746Z",
          "shell.execute_reply": "2022-02-04T19:00:03.351096Z"
        },
        "trusted": true
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = get_quality(X[0], y_train, X[1], y_test, alert=True)\n",
        "# Test MAE for Ridge: 0.9620847507977837\n",
        "# Train MAE for Ridge: 0.954713798120569"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T19:00:03.353765Z",
          "iopub.execute_input": "2022-02-04T19:00:03.354007Z",
          "iopub.status.idle": "2022-02-04T19:00:07.521533Z",
          "shell.execute_reply.started": "2022-02-04T19:00:03.353978Z",
          "shell.execute_reply": "2022-02-04T19:00:07.520519Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCya78R2vfAS",
        "outputId": "82ec068c-087b-4756-a6af-4e9cd1f8d3a3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE for Ridge: 0.9641755105243773\n",
            "Train MAE for Ridge: 0.9556134385176874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проведите эксперименты с размерностью эмбеддинга. Для каждого из двух методов постройте график зависимости качества модели от размерности эмбеддинга. "
      ],
      "metadata": {
        "id": "4s-6HQo0XHaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dims = [i * 50 for i in range(6, 11)]\n",
        "results = {\n",
        "    'mean': [],\n",
        "    'tfidf': []\n",
        "}\n",
        "\n",
        "for dim in tqdm(emb_dims):\n",
        "    \n",
        "    w2v_model_pos = Word2Vec(df_train['positive'], window=30, vector_size=dim)\n",
        "    w2v_model_neg = Word2Vec(df_train['negative'], window=30, vector_size=dim)\n",
        "    \n",
        "    vec1, vec2 = mean_vectorizer(w2v_model_pos), mean_vectorizer(w2v_model_neg)\n",
        "    X, _, _ = transform_fragments(vec1, vec2, [df_train, df_test])\n",
        "    mv_q_rig = get_quality(X[0], y_train, X[1], y_test)\n",
        "    results['mean'].append(mv_q_rig)\n",
        "    \n",
        "    vec1, vec2 = tfidf_vectorizer(w2v_model_pos), tfidf_vectorizer(w2v_model_neg)\n",
        "    X, _, _ = transform_fragments(vec1, vec2, [df_train, df_test])\n",
        "    tfidf_q_rig = get_quality(X[0], y_train, X[1], y_test)\n",
        "    results['tfidf'].append(tfidf_q_rig)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T19:00:07.523129Z",
          "iopub.execute_input": "2022-02-04T19:00:07.523359Z",
          "iopub.status.idle": "2022-02-04T19:24:58.327477Z",
          "shell.execute_reply.started": "2022-02-04T19:00:07.523331Z",
          "shell.execute_reply": "2022-02-04T19:24:58.326458Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5eddabf98c18417b81e22ebf6a330cc1",
            "6aaa62a5c1b042e6887deba22d8bfb75",
            "909acff990ca4b958f9b852f32310f80",
            "41a484bbeac84f4cb6bc2ebb9a0dc72c",
            "0f00f02a2f3248638ff87a1c28f51bc9",
            "187d454ecbf849b8b68bc4cc7088e393",
            "079375f382b4401f821766e6f50333b7",
            "69976f23fa994e2bbdc9e0114eb3fca4",
            "e17bec2e0772435b82ae8fd173e62271",
            "257de7cab68849689317b27a77440e6f",
            "d314dd9a7ae942fabea043896892f754"
          ]
        },
        "id": "uAy5dRMIvfAS",
        "outputId": "70e6f165-0b5e-4b8d-d3b8-9356e5210526"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5eddabf98c18417b81e22ebf6a330cc1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n",
        "ax1.plot(emb_dims, results['mean'], color='lime', label='mean')\n",
        "ax1.plot(emb_dims, results['tfidf'], color='blue', label='IDF mean')\n",
        "ax1.set_xlabel('embegging size')\n",
        "ax1.set_ylabel('MAE')\n",
        "ax1.set_title('Embegging size - MAE')\n",
        "ax1.legend(shadow=False, fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T19:24:58.331313Z",
          "iopub.execute_input": "2022-02-04T19:24:58.331592Z",
          "iopub.status.idle": "2022-02-04T19:24:58.631994Z",
          "shell.execute_reply.started": "2022-02-04T19:24:58.331544Z",
          "shell.execute_reply": "2022-02-04T19:24:58.631101Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "fQZ9mH3JvfAS",
        "outputId": "3acf4201-e202-41e4-cc38-1dc8731e826c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results['tfidf'][np.argmin(results['tfidf'])], emb_dims[np.argmin(results['tfidf'])], \\\n",
        "results['mean'][np.argmin(results['mean'])], emb_dims[np.argmin(results['mean'])]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T19:24:58.633168Z",
          "iopub.execute_input": "2022-02-04T19:24:58.633410Z",
          "iopub.status.idle": "2022-02-04T19:24:58.641558Z",
          "shell.execute_reply.started": "2022-02-04T19:24:58.633380Z",
          "shell.execute_reply": "2022-02-04T19:24:58.640702Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DATIU9dovfAT",
        "outputId": "5b6bc9ad-c004-45fa-f742-3d487c522dd3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9412967344933015, 500, 0.9506049075731786, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Сделайте выводы:\n",
        "Я позапускал много раз. Оптимальный размер во многом зависит от гиперпараметров, например, от размера окна `window`. Тенденции тоже могут различаться, но я заметил, что оптимальный размер эмбеддинга обычно свыше 300."
      ],
      "metadata": {
        "id": "dLBbuIOJvfAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del w2v_model_neg, w2v_model_pos, results"
      ],
      "metadata": {
        "id": "WvtrDAyND0mP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь попробуйте обучить логистическую или линейную регрессию на любых других эмбеддингах размерности 300 и сравните качество с Word2Vec."
      ],
      "metadata": {
        "id": "f29vizrmXHaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "model_ft_pos = FastText(df_train['positive'], window=10, epochs=12, alpha=0.09, vector_size=300)\n",
        "model_ft_neg = FastText(df_train['negative'], window=10, epochs=12,  alpha=0.09, vector_size=300)\n",
        "vec1, vec2 = tfidf_vectorizer(model_ft_pos), tfidf_vectorizer(model_ft_neg)\n",
        "X, _, _ = transform_fragments(vec1, vec2, [df_train, df_test])\n",
        "_ = get_quality(X[0], y_train, X[1], y_test, alert=True, alpha=0.2)\n",
        "\n",
        "# Test MAE for Ridge: 0.9349474496674942\n",
        "# Train MAE for Ridge: 0.9270191574689883"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T19:24:58.642966Z",
          "iopub.execute_input": "2022-02-04T19:24:58.643214Z",
          "iopub.status.idle": "2022-02-04T19:30:20.315496Z",
          "shell.execute_reply.started": "2022-02-04T19:24:58.643185Z",
          "shell.execute_reply": "2022-02-04T19:30:20.314776Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd3ak5TGvfAT",
        "outputId": "c3df993d-ed56-4e2c-e057-2cede6a885ed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE for Ridge: 0.9392109693955217\n",
            "Train MAE for Ridge: 0.9283219987028422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents_pos = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_train['positive'])]\n",
        "documents_neg = [TaggedDocument(doc, [i]) for i, doc in enumerate(df_train['negative'])]\n",
        "model_d2v_pos = Doc2Vec(documents_pos, window=7, epochs=15, alpha=0.05, vector_size=300)\n",
        "model_d2v_neg = Doc2Vec(documents_neg, window=7, epochs=10, alpha=0.05, vector_size=300)\n",
        "del documents_pos, documents_neg\n",
        "\n",
        "vec1, vec2 = tfidf_vectorizer(model_d2v_pos), tfidf_vectorizer(model_d2v_neg)\n",
        "X, _, _ = transform_fragments(vec1, vec2, [df_train, df_test])\n",
        "_ = get_quality(X[0], y_train, X[1], y_test, alert=True)\n",
        "\n",
        "# Test MAE for Ridge: 0.9338142101887302\n",
        "# Train MAE for Ridge: 0.9249547038064014"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T19:31:48.911855Z",
          "iopub.execute_input": "2022-02-04T19:31:48.912070Z",
          "iopub.status.idle": "2022-02-04T19:33:34.751481Z",
          "shell.execute_reply.started": "2022-02-04T19:31:48.912044Z",
          "shell.execute_reply": "2022-02-04T19:33:34.750539Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksyvI-LyvfAT",
        "outputId": "7c39b114-3a39-4034-b6d7-cda5d02e4c05"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE for Ridge: 0.9323742092343171\n",
            "Train MAE for Ridge: 0.9254249373820057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Выводы:\n",
        "Не лучше, чем word2vec + Ridge + IGF-взвешивание. Если сравнивать эти два метода, то из них лучше себя показывает IDF-взвешивание + Ridge + doc2vec."
      ],
      "metadata": {
        "id": "u6dLqWYZvfAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предскажите вашей лучшей моделью из этого задания тестовые данные из [соревнования](https://www.kaggle.com/t/3e8fa6cec6d048bf8e93fb72e441d88c) и сделайте сабмит. Какой у вас получился скор? Прикрепите скриншот из кэггла."
      ],
      "metadata": {
        "id": "9AjabHMsXXBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit_df = pd.read_csv(PATH_TO_TEST_DATA)\n",
        "\n",
        "X_subm, _, _ = transform_fragments(vec1, vec2, [submit_df], mode=2)\n",
        "rig = Ridge().fit(X[0], y_train)\n",
        "y_pred_rig_subm = rig.predict(X_subm[0])\n",
        "submit_df.drop(columns=['negative', 'positive'], inplace=True)\n",
        "submit_df['score'] = y_pred_rig_subm\n",
        "\n",
        "submit_df.to_csv('sumbit_2.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T19:33:34.753045Z",
          "iopub.execute_input": "2022-02-04T19:33:34.753431Z",
          "iopub.status.idle": "2022-02-04T19:34:58.966554Z",
          "shell.execute_reply.started": "2022-02-04T19:33:34.753367Z",
          "shell.execute_reply": "2022-02-04T19:34:58.965923Z"
        },
        "trusted": true,
        "id": "MFZMnGJFvfAU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_subm, X, vec1, vec2, model_ft_pos, model_ft_neg, model_d2v_pos, model_d2v_neg"
      ],
      "metadata": {
        "id": "IzgWUvpTEXKO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Часть 3. 4 балла"
      ],
      "metadata": {
        "id": "EO5TZriLXHaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь давайте воспользуемся более продвинутыми методами обработки текстовых данных, которые мы проходили в нашем курсе. Обучите RNN/Transformer для предсказания пользовательской оценки."
      ],
      "metadata": {
        "id": "5RNngNdWXHaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если будете обучать RNN, попробуйте ограничить максимальную длину предложения. Некоторые отзывы могут быть слишком длинные относительно остальных.\n",
        "\n",
        "Чтобы пользоваться DataLoader, все его элементы должны быть одинаковой размерности. Для этого вы можете добавить нулевой паддинг ко всем предложениям (см пример pad_sequence)"
      ],
      "metadata": {
        "id": "_8YdTedQXHaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade torch"
      ],
      "metadata": {
        "id": "tsQLleTIAkJW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torchtext\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "89Y9wsViXHaU",
        "execution": {
          "iopub.status.busy": "2022-02-04T07:31:43.632037Z",
          "iopub.execute_input": "2022-02-04T07:31:43.632373Z",
          "iopub.status.idle": "2022-02-04T07:31:43.637542Z",
          "shell.execute_reply.started": "2022-02-04T07:31:43.632333Z",
          "shell.execute_reply": "2022-02-04T07:31:43.636824Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebeef4c9-6cef-4b03-a157-b3351a088348"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.2+cu102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вообще говоря можно было бы удалить те отзывы, где в полях пусто, или же с заполнениями а-ля `no positive`/`no negative`. Люди склонны ничего не писать и ставить высокие оценки, либо же писать `no positive` и ставить 0 или, напротив, писать `no negative` и ставить 10. Поэтому я решил ничего не удалять - такие данные могут сильно влиять на результат."
      ],
      "metadata": {
        "id": "RjtleSFOvfAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Fine-Tuning"
      ],
      "metadata": {
        "id": "8KrXqRE6vfAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я буду решать задачу регрессии, и в таких случаях принято скалировать целевую переменную."
      ],
      "metadata": {
        "id": "fNxJ2ffnvfAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cqW7tXo_Bbs3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "PATH_TO_TRAIN_DATA = 'drive/MyDrive/data/train.csv'\n",
        "PATH_TO_TEST_DATA = 'drive/MyDrive/data/test.csv'\n",
        "\n",
        "df = pd.read_csv(PATH_TO_TRAIN_DATA)\n",
        "data = df['positive'] + ' ' + df['negative']\n",
        "df_train, df_test, y_train, y_test = train_test_split(data, df['score'], random_state=1412)\n",
        "\n",
        "scaler = StandardScaler().fit(y_train.to_numpy().reshape(-1, 1))\n",
        "y_train_ = scaler.transform(y_train.to_numpy().reshape(-1, 1))\n",
        "y_test_ = scaler.transform(y_test.to_numpy().reshape(-1, 1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T20:31:21.944130Z",
          "iopub.execute_input": "2022-02-04T20:31:21.944416Z",
          "iopub.status.idle": "2022-02-04T20:31:21.953153Z",
          "shell.execute_reply.started": "2022-02-04T20:31:21.944387Z",
          "shell.execute_reply": "2022-02-04T20:31:21.952208Z"
        },
        "trusted": true,
        "id": "3B3hOVQXvfAX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация и преобразование данных"
      ],
      "metadata": {
        "id": "onqU8Q2lpsD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прочитав несколько статей, я понял, что нужно использовать встроенные методы эмбеддинга:\n",
        "- \"We can’t use the pre-tokenized version because, in order to apply the pre-trained BERT, we must use the tokenizer provided by the model. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words.\"\n",
        "\n",
        "- \"Common embedding approaches in NLP such as Word2Vec or FastText usually require lemmatization and stop words to be removed, but this is not the case for BERT. The general structure of the text should actually not be modified since BERT relies on it to learn and interpret context.\"\n",
        "\n",
        "Как вообще я хотел бы преобразовать данные: отдельно лемматизируем и токенизируем столбцы `positive` и `negative`, получаем тензоры, затем конкатенируем. Можно преобразовывать предварительно сконкатенированные тексты отзывов, а затем токенизировать. В первом случае придется учить две модели и каким-то образом ассемблировать, поэтому я склоняюсь ко второму варианту, хоть и считаю его не совсем правильным."
      ],
      "metadata": {
        "id": "J0U_eOysvfAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Привожу выдержки из некоторых прочитанных статей. Здесь описываются специальные токены и в каком виде мы должны подавать данные на вход модели.\n",
        "\n",
        "We are required to:\n",
        "\n",
        "* Add special tokens to the start and end of each sentence.\n",
        "* Pad & truncate all sentences to a single constant length.\n",
        "* Explicitly differentiate real tokens from padding tokens with the “attention mask”.\n",
        "\n",
        "\n",
        "[SEP]\n",
        "\n",
        "At the end of every sentence, we need to append the special [SEP] token.\n",
        "\n",
        "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?).\n",
        "\n",
        "I am not certain yet why the token is still required when we have only single-sentence input, but it is!\n",
        "\n",
        "[CLS]\n",
        "\n",
        "For classification tasks, we must prepend the special [CLS] token to the beginning of every sentence.\n",
        "\n",
        "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!).\n",
        "\n",
        "\n",
        "\\\\\n",
        "BERT takes as input sequences of equal length. Input sequences must thus be padded or truncated at a given length with special characters explicitly indicating the actual start and end of the sequence. The special start character is referred to as the “[CLS]” token.\n",
        "These input sequences are then split into two :\n",
        "- A sequence of “input ids”, mapping the words to tokens from the vocabulary the model was pre-trained with,\n",
        "- A binary sequence of “attention masks” that indicate whether the “input id” at a given index is a word or padding."
      ],
      "metadata": {
        "id": "plYEXdAponJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers"
      ],
      "metadata": {
        "id": "AwM6TaoP1w9j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel, BertConfig\n",
        "model_name = \"bert-base-uncased\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T21:36:53.097344Z",
          "iopub.execute_input": "2022-02-04T21:36:53.100899Z",
          "iopub.status.idle": "2022-02-04T21:36:53.110729Z",
          "shell.execute_reply.started": "2022-02-04T21:36:53.100786Z",
          "shell.execute_reply": "2022-02-04T21:36:53.109888Z"
        },
        "trusted": true,
        "id": "euOB0OBGvfAX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Важно подобрать максимальную длину входной последовательность `max_length`. Максимально допустимо для модели - 512, поэтому посчитаем длины и посмотрим, сколько у нас отзывов, превышающих этот порог."
      ],
      "metadata": {
        "id": "1iPUEV20vfAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tkn = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "def define_max_length(dataframe):\n",
        "    \n",
        "    tmp_data = np.array(tkn(dataframe.tolist(), return_length=True)['length'])\n",
        "    # берем 510, так как у нас в запасе два специальных токена - начала и конца последовательности\n",
        "    inds = np.argwhere(tmp_data > 510)\n",
        "    count = len(inds)\n",
        "    max_length = np.max(tmp_data[inds])\n",
        "    return max_length, count, inds.reshape(-1)\n",
        "\n",
        "train = define_max_length(df_train)\n",
        "test = define_max_length(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp5w1dPFQkGL",
        "outputId": "c8815961-a304-4950-e0b5-af2870765259"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('For train set:\\n\\tMax Length %.2f\\n\\tCount: %.2f' % (train[0], train[1]))\n",
        "print('For test set:\\n\\tMax Length %.2f\\n\\tCount: %.2f' % (test[0], test[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOnYZ2FASGJu",
        "outputId": "843c1ef9-74a5-4853-fb26-d78ca5e728f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For train set:\n",
            "\tMax Length 663.00\n",
            "\tCount: 10.00\n",
            "For test set:\n",
            "\tMax Length 519.00\n",
            "\tCount: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Доля таких отзывов относительно числа объектов в датасетах крайне мала, поэтому я предлагаю их выкинуть."
      ],
      "metadata": {
        "id": "s-UMLp4lV7WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_ind = train[2][1]\n",
        "print(y_train_[check_ind-1], y_train_[check_ind], y_train_[check_ind+1], sep=\"\\n\")\n",
        "df_train.iloc[[check_ind-1, check_ind, check_ind+1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MFApoSjWJbA",
        "outputId": "b19d6a25-b142-4977-80ff-0d4cc8a4d942"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.24375773]\n",
            "[-0.79355767]\n",
            "[-0.54948346]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15825     Location spacious rooms  Absolutely boring br...\n",
              "20862     We drove to the Hotel and paid the extra to p...\n",
              "66693     Good location   The decor is a little tired t...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop(index=df_train.index[train[2]], axis=0, inplace=True)\n",
        "df_test.drop(index=df_test.index[test[2]], axis=0, inplace=True)\n",
        "\n",
        "y_train_ = y_train_[np.setdiff1d([i for i in range(len(y_train_))], train[2])]\n",
        "y_test_ = y_test_[np.setdiff1d([i for i in range(len(y_test_))], test[2])]\n",
        "df_train.shape, df_test.shape, len(y_train_), len(y_test_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irfTtgiTbCZ6",
        "outputId": "b6f6d253-bfa3-4495-d79d-950be242deb9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((74990,), (24999,), 74990, 24999)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_ind = train[2][1]\n",
        "print(y_train_[check_ind-2], y_train_[check_ind-1], y_train_[check_ind], sep=\"\\n\")\n",
        "df_train.iloc[[check_ind-2, check_ind-1, check_ind]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy7UeeYTbERE",
        "outputId": "f4bd0ec8-2b16-415b-a2f0-ce639c9fb030"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.24375773]\n",
            "[-0.54948346]\n",
            "[-0.79355767]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15825     Location spacious rooms  Absolutely boring br...\n",
              "66693     Good location   The decor is a little tired t...\n",
              "29380                          Clean room  Far from center\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все ОК. Можно токенизировать."
      ],
      "metadata": {
        "id": "ofzpZA5qbcHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
        "\n",
        "# output_attentions=True\n",
        "train_corpus = tokenizer(text=df_train.tolist(),\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first', # можно указать truncation=False, но лучше не буду, вдруг бага какая\n",
        "                            max_length=512,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "\n",
        "test_corpus = tokenizer(text=df_test.tolist(),\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=512,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "\n",
        "input_ids_train = train_corpus['input_ids']\n",
        "attention_mask_train = train_corpus['attention_mask']\n",
        "\n",
        "input_ids_test = test_corpus['input_ids']\n",
        "attention_mask_test = test_corpus['attention_mask']\n",
        "\n",
        "# sentence split\n",
        "# tokenizer.tokenize('str')\n",
        "# sentence mapped to token ids\n",
        "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize('str))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-04T21:36:58.798456Z",
          "iopub.execute_input": "2022-02-04T21:36:58.799762Z",
          "iopub.status.idle": "2022-02-04T21:37:18.919266Z",
          "shell.execute_reply.started": "2022-02-04T21:36:58.799697Z",
          "shell.execute_reply": "2022-02-04T21:37:18.917454Z"
        },
        "trusted": true,
        "id": "ufAM7sScvfAX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_train.shape, attention_mask_train.shape, input_ids_test.shape, attention_mask_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSnEnAzAvfAY",
        "outputId": "7e39b7fa-7aab-422d-b8c5-0f37b54664b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([74990, 512]),\n",
              " torch.Size([74990, 512]),\n",
              " torch.Size([24999, 512]),\n",
              " torch.Size([24999, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, данные были приведены к соответствующему виду - максимальный размер 512, были добавлены соответствующие токены:"
      ],
      "metadata": {
        "id": "bn1mgVWMnDf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_train[0][:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10WvRTzgnMqL",
        "outputId": "47c52591-35ed-4bc7-faad-00404bb49e46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  2053,  3893,  1996,  2282,  2001,  4714,  1998,  1996,  3976,\n",
              "         4654, 16368, 16313,  4630,  2005,  2008,  2946,  2282,   102,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask_train[0][:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3rTZ1sBnUCL",
        "outputId": "719e9f9c-b1a8-413a-ae82-2b77a5089237"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Индексы `101` и `102` - токены начала `[CLS]` и конца `[SEP]` последовательности. Нули - токены паддинга `[PAD]`. Единицы в маске внимания стоят там, где вообще находится последовательность, т.е. на паддинги она не реагирует. Иными словами маска внимания отличает паддинг от остальных токенов."
      ],
      "metadata": {
        "id": "IhSX2oyTvfAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузка данных"
      ],
      "metadata": {
        "id": "qSB0ITwEpla4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "targets_train = torch.tensor(y_train_, dtype=torch.int32)\n",
        "targets_test = torch.tensor(y_test_, dtype=torch.int32)\n",
        "\n",
        "train_dataset = TensorDataset(input_ids_train, attention_mask_train, targets_train)\n",
        "test_dataset = TensorDataset(input_ids_test, attention_mask_test, targets_test)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "CgDhiiq-pyHo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Модель"
      ],
      "metadata": {
        "id": "jtpzQR_0sS5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На выходе последнего (12-го) слоя трансформера в задаче классификации для предсказания используется только первый эмбеддинг `[CLS]`. Выглядит это [так](http://www.mccormickml.com/assets/BERT/padding_and_mask.png). \n",
        "\n",
        "- “The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks,\" - из оригинальной статьи.\n",
        "\n",
        "Для задачи регрессии мы будем использовать практически то же самое, но мы добавим полносвязный слой."
      ],
      "metadata": {
        "id": "62gaLG9csVkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertRegressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        # FC layer for fine-tuning\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(768, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_masks):\n",
        "        outputs = self.bert(input_ids, attention_masks)\n",
        "        class_label_output = outputs[1]\n",
        "        outputs = self.regressor(class_label_output)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "i3EpNVoYznhA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def device_memory_info(device):\n",
        "    if device.type == 'cuda':\n",
        "        print(torch.cuda.get_device_name(0))\n",
        "        print('Memory Usage:')\n",
        "        print('Allocated:', round(torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
        "        print('Cached:   ', round(torch.cuda.memory_reserved(0) / 1024 ** 3, 1), 'GB')\n",
        "\n",
        "device_memory_info(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCEhFyh51eZ4",
        "outputId": "b26e03c3-72f4-4169-ee5e-c0c225ab6d29"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertRegressor()\n",
        "model.to(device)\n",
        "model_device = next(model.parameters()).device\n",
        "model_device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCHC2IdyRyVd",
        "outputId": "9d496c83-dac5-453f-eed7-af81a79937ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_memory_info(model_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6uLfnbcR2vn",
        "outputId": "11509dc2-8ab5-4cf6-d92e-8298e1d4c8a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.4 GB\n",
            "Cached:    0.5 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Оптимайзер, функция потерь, планировщик"
      ],
      "metadata": {
        "id": "NN9d-HX-1tjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr=5e-5\n",
        "eps=1e-8\n",
        "epochs=3\n",
        "total_steps = len(train_loader) * epochs\n",
        "total_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx4HXry_ELcz",
        "outputId": "28671950-af87-4485-e9a0-bbe84e151fcd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14061"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/optimizer_schedules\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "optimizer = AdamW(model.parameters(), lr=lr, eps=eps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "criterion = nn.L1Loss(reduction='sum') # MAE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhFfkoA611Kl",
        "outputId": "b13f2334-8c09-446e-cd88-2363c74abbf2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возможные гиперпараметры:\n",
        "- `batch_size`: 16, 32\n",
        "- `lr`: 5e-5, 3e-5, 2e-5\n",
        "- `epochs`: 2, 3, 4"
      ],
      "metadata": {
        "id": "j7g4Tvl74s-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Оптимизации"
      ],
      "metadata": {
        "id": "LkTz-1QF5lJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Очень классной идеей было бы использовать Mixed Precision Training. Мы разбирали этот метод на на НИСе, и он позволяет ускорить обучение. Я наткнулся на библиотеку, которая позволяет это сделать легко и просто: [здесь](https://github.com/nvidia/apex).\n",
        "\n",
        "Человек, который написал код ниже для установки библиотеки - просто святейший."
      ],
      "metadata": {
        "id": "N-2Yph846EOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile setup.sh\n",
        "\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# cd apex\n",
        "# pip install -v --no-cache-dir ./"
      ],
      "metadata": {
        "id": "AOSBsgq29Hq6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !sh setup.sh"
      ],
      "metadata": {
        "id": "Q9kcXkHiLfCV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from apex import amp"
      ],
      "metadata": {
        "id": "H_jTneAw9ecw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_level = 'O1'\n",
        "loss_scale = 128.0\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level, loss_scale=loss_scale)"
      ],
      "metadata": {
        "id": "WtRd08Z46dgq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Жаль, что в наличие всего одна GPU, так бы можно было заюзать `DistributedDataParallel`."
      ],
      "metadata": {
        "id": "aArvYYMAO57x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение"
      ],
      "metadata": {
        "id": "ZoRXiukgG1Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.clip_grad import clip_grad_norm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def plot_graphs(metrics_dict):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(18, 6))\n",
        "    ax.plot(metrics_dict['Epoch'], metrics_dict['Train Loss'], 'darkorchid', label='Train', linewidth=2)\n",
        "    ax.plot(metrics_dict['Epoch'], metrics_dict['Valid Loss'], 'crimson', label='Valid', linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Loss - Epoch')\n",
        "    ax.legend(shadow=False, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_loop(model, optimizer, criterion, scheduler, train_loader, valid_loader, device='cpu', epochs=3):\n",
        "    metrics_dict = { 'Epoch': [i + 1 for i in range(epochs)], 'Train Loss': [], 'Valid Loss': [] }\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            ids, masks, targets = tuple(t.to(device) for t in batch)\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(ids, masks)           \n",
        "            loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "\n",
        "            loss.backward()\n",
        "            # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "            #     scaled_loss.backward()\n",
        "\n",
        "            clip_grad_norm(model.parameters(), max_norm=2)            \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_loss += loss.item() * len(targets)\n",
        "                \n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        metrics_dict['Train Loss'].append(train_loss)\n",
        "        \n",
        "        \n",
        "        # model.eval()\n",
        "        # valid_loss = 0.0\n",
        "        # for batch_v in valid_loader:\n",
        "        #     ids_v, masks_v, target_v = tuple(t.to(device) for t in batch_v)\n",
        "\n",
        "        #     with torch.no_grad():\n",
        "        #         outputs_v = model(ids_v['input_ids'], ids_v['attention_mask'])           \n",
        "        #         loss = criterion(outputs_v.squeeze(), target_v.squeeze())\n",
        "        #     valid_loss += loss.item() * len(target_v)\n",
        "                \n",
        "        # valid_loss /= len(valid_loader.dataset)\n",
        "        # metrics_dict['Valid Loss'].append(valid_loss)\n",
        "    \n",
        "        clear_output(wait=True)\n",
        "        display(pd.DataFrame(metrics_dict))\n",
        "        \n",
        "    plot_graphs(metrics_dict)"
      ],
      "metadata": {
        "id": "H1U793qiG6Xc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "device_memory_info(device)\n",
        "# train_loop(model, optimizer, criterion, scheduler, train_loader, test_loader, device=model_device, epochs=1)\n",
        "train(model, optimizer, scheduler, criterion, 1, train_loader, model_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "i7woESJBPgVH",
        "outputId": "e2366deb-4677-42f0-ef05-74c1038d156a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.4 GB\n",
            "Cached:    0.5 GB\n",
            "0\n",
            "-----\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-260acda8059c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice_memory_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train_loop(model, optimizer, criterion, scheduler, train_loader, test_loader, device=model_device, epochs=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-a1551771a1ea>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, сriterion, epochs, train_dataloader, device, clip_value)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m                                \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             loss = сriterion(outputs.squeeze(), \n\u001b[1;32m     14\u001b[0m                              batch_labels.squeeze())\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-589823b4347a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_masks)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mclass_label_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_label_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 )\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         )\n\u001b[1;32m    479\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.54 GiB already allocated; 128.81 MiB free; 10.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "train_loss = 0.0\n",
        "for batch in train_loader:\n",
        "    print(0)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    ids, masks, targets = tuple(t.to(device) for t in batch)\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    print(1)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    model.zero_grad()\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    print(2)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    outputs = model(ids, masks)\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    print(3)\n",
        "    print('------------------------------------------------------------------------------------')        \n",
        "    loss = criterion(outputs.squeeze(), targets.squeeze())\n",
        "    # del outputs\n",
        "    # torch.cuda.empty_cache()\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    print(4)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    loss.backward()\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    print(5)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    clip_grad_norm(model.parameters(), max_norm=2)\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    print(6)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    optimizer.step()\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    print(7)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    scheduler.step()\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    print(8)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    train_loss += loss.item() * len(targets)\n",
        "    device_memory_info(device)\n",
        "    print('------------------------------------------------------------------------------------')\n",
        "    break"
      ],
      "metadata": {
        "id": "l1p1fe6aed9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device_memory_info(device)"
      ],
      "metadata": {
        "id": "RHo7z_5GgU7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предсказания"
      ],
      "metadata": {
        "id": "tfewVaphVaDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    output = []\n",
        "    for batch in dataloader:\n",
        "        batch_inputs, batch_masks = tuple(b.to(device) for b in batch)\n",
        "        with torch.no_grad():\n",
        "            output += model(batch_inputs, batch_masks).view(1,-1).tolist()[0]\n",
        "    return output"
      ],
      "metadata": {
        "id": "N5rfKxv-NRKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_df = pd.read_csv(PATH_TO_TEST_DATA)\n",
        "submit_data = submit_df['positive'] + ' ' + submit_df['negative']\n",
        "\n",
        "submit_corpus = tokenizer(text=df_test.tolist(),\n",
        "                            add_special_tokens=True,\n",
        "                            padding='max_length',\n",
        "                            truncation='longest_first',\n",
        "                            max_length=512,\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt')\n",
        "\n",
        "input_ids_submit = submit_corpus['input_ids']\n",
        "attention_mask_submit = submit_corpus['attention_mask']\n",
        "input_ids_submit.shape, attention_mask_submit.shape"
      ],
      "metadata": {
        "id": "DqE3RcLQNceA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_dataset = TensorDataset(input_ids_submit, attention_mask_submit)\n",
        "submit_loader = DataLoader(submit_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "y_pred_scaled = predict(model, submit_loader, device)\n",
        "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
        "\n",
        "submit_df.drop(columns=['negative', 'positive'], inplace=True)\n",
        "submit_df['score'] = y_pred\n",
        "submit_df.to_csv('transformer_sumbit.csv', index=False)"
      ],
      "metadata": {
        "id": "_mPQ8eeJYlKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Контест (до 3 баллов)\n",
        "\n",
        "По итогам всех ваших экспериментов выберите модель, которую считаете лучшей. Сделайте сабмит в контест. В зависимости от вашего скора на публичном лидерборде, мы начислим вам баллы:\n",
        "\n",
        " - <0.76 - 3 балла\n",
        " - [0.76; 0.78) - 2 балла\n",
        " - [0.78; 0.8) - 1 балл"
      ],
      "metadata": {
        "id": "n3OeNQkoXHaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jfORFaucXHaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}